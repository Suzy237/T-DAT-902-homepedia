Starting nginx: nginx.
[09-Jul-2024 16:23:04] NOTICE: fpm is running, pid 20
[09-Jul-2024 16:23:04] NOTICE: ready to handle connections
[2024-07-09 18:24:08] production.INFO: Executing Python script: /var/www/python_scripts/process_housing_market_data.py  
24/07/09 16:24:10 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
[Stage 0:>                                                          (0 + 1) / 1]                                                                                [Stage 1:>                                                        (0 + 12) / 12][Stage 1:====>                                                    (1 + 11) / 12][Stage 1:=========>                                               (2 + 10) / 12][Stage 1:===================>                                      (4 + 8) / 12][Stage 1:===========================================>              (9 + 3) / 12]                                                                                [Stage 5:>                                                        (0 + 10) / 10][Stage 5:=====>                                                    (1 + 9) / 10][Stage 5:=================>                                        (3 + 7) / 10][Stage 5:=============================>                            (5 + 5) / 10][Stage 5:==================================>                       (6 + 4) / 10][Stage 5:========================================>                 (7 + 3) / 10]                                                                                [Stage 7:>                                                          (0 + 1) / 1]                                                                                [Stage 8:>                                                        (0 + 12) / 12][Stage 8:====>                                                    (1 + 11) / 12][Stage 8:=========>                                               (2 + 10) / 12][Stage 8:==============>                                           (3 + 9) / 12][Stage 8:===================>                                      (4 + 8) / 12][Stage 8:=============================>                            (6 + 6) / 12][Stage 8:======================================>                   (8 + 4) / 12][Stage 8:===========================================>              (9 + 3) / 12]                                                                                24/07/09 16:25:48 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.
[Stage 9:>                                                        (0 + 10) / 10][Stage 9:=====>                                                    (1 + 9) / 10][Stage 9:=================>                                        (3 + 7) / 10]                                                                                [Stage 10:>                                                         (0 + 2) / 2][Stage 10:=============================>                            (1 + 1) / 2]                                                                                Traceback (most recent call last):
  File "/var/www/python_scripts/process_housing_market_data.py", line 145, in <module>
    .save()
     ^^^^^^
  File "/usr/local/lib/python3.11/dist-packages/pyspark/sql/readwriter.py", line 1461, in save
    self._jwrite.save()
  File "/usr/local/lib/python3.11/dist-packages/py4j/java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
                   ^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/dist-packages/pyspark/errors/exceptions/captured.py", line 179, in deco
    return f(*a, **kw)
           ^^^^^^^^^^^
  File "/usr/local/lib/python3.11/dist-packages/py4j/protocol.py", line 326, in get_return_value
    raise Py4JJavaError(
py4j.protocol.Py4JJavaError: An error occurred while calling o215.save.
: java.lang.NoClassDefFoundError: com/mongodb/WriteConcern
	at com.mongodb.spark.sql.connector.config.WriteConfig.createWriteConcern(WriteConfig.java:347)
	at com.mongodb.spark.sql.connector.config.WriteConfig.<init>(WriteConfig.java:256)
	at com.mongodb.spark.sql.connector.config.MongoConfig.writeConfig(MongoConfig.java:81)
	at com.mongodb.spark.sql.connector.config.MongoConfig.toWriteConfig(MongoConfig.java:276)
	at com.mongodb.spark.sql.connector.MongoTable.newWriteBuilder(MongoTable.java:114)
	at org.apache.spark.sql.execution.datasources.v2.V2Writes$.org$apache$spark$sql$execution$datasources$v2$V2Writes$$newWriteBuilder(V2Writes.scala:145)
	at org.apache.spark.sql.execution.datasources.v2.V2Writes$$anonfun$apply$1.applyOrElse(V2Writes.scala:63)
	at org.apache.spark.sql.execution.datasources.v2.V2Writes$$anonfun$apply$1.applyOrElse(V2Writes.scala:44)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)
	at org.apache.spark.sql.execution.datasources.v2.V2Writes$.apply(V2Writes.scala:44)
	at org.apache.spark.sql.execution.datasources.v2.V2Writes$.apply(V2Writes.scala:40)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
	at scala.collection.immutable.List.foldLeft(List.scala:91)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$optimizedPlan$1(QueryExecution.scala:152)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan$lzycompute(QueryExecution.scala:148)
	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan(QueryExecution.scala:144)
	at org.apache.spark.sql.execution.QueryExecution.assertOptimized(QueryExecution.scala:162)
	at org.apache.spark.sql.execution.QueryExecution.executedPlan$lzycompute(QueryExecution.scala:182)
	at org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:179)
	at org.apache.spark.sql.execution.QueryExecution.simpleString(QueryExecution.scala:238)
	at org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$explainString(QueryExecution.scala:284)
	at org.apache.spark.sql.execution.QueryExecution.explainString(QueryExecution.scala:252)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:117)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)
	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)
	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)
	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:859)
	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:319)
	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:248)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:568)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: java.lang.ClassNotFoundException: com.mongodb.WriteConcern
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:445)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:592)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:525)
	... 80 more
[2024-07-09 18:25:54] production.ERROR: Failed to execute Python script:  {"output":["root"," |-- CODGEO_2023: string (nullable = true)"," |-- annee: integer (nullable = true)"," |-- classe: string (nullable = true)"," |-- unité.de.compte: string (nullable = true)"," |-- valeur.publiée: string (nullable = true)"," |-- faits: string (nullable = true)"," |-- tauxpourmille: string (nullable = true)"," |-- complementinfoval: string (nullable = true)"," |-- complementinfotaux: string (nullable = true)"," |-- POP: integer (nullable = true)"," |-- millPOP: integer (nullable = true)"," |-- LOG: string (nullable = true)"," |-- millLOG: integer (nullable = true)","","root"," |-- code_commune_INSEE: string (nullable = true)"," |-- nom_commune_postal: string (nullable = true)"," |-- code_postal: integer (nullable = true)"," |-- libelle_acheminement: string (nullable = true)"," |-- ligne_5: string (nullable = true)"," |-- latitude: double (nullable = true)"," |-- longitude: double (nullable = true)"," |-- code_commune: integer (nullable = true)"," |-- article: string (nullable = true)"," |-- nom_commune: string (nullable = true)"," |-- nom_commune_complet: string (nullable = true)"," |-- code_departement: string (nullable = true)"," |-- nom_departement: string (nullable = true)"," |-- code_region: integer (nullable = true)"," |-- nom_region: string (nullable = true)","","root"," |-- Date mutation: string (nullable = true)"," |-- Nature mutation: string (nullable = true)"," |-- Valeur fonciere: string (nullable = true)"," |-- No voie: double (nullable = true)"," |-- Code voie: string (nullable = true)"," |-- Code postal: double (nullable = true)"," |-- Commune: string (nullable = true)"," |-- Code departement: string (nullable = true)"," |-- Code commune: integer (nullable = true)"," |-- Section: string (nullable = true)"," |-- No plan: integer (nullable = true)"," |-- Nombre de lots: integer (nullable = true)"," |-- Type local: string (nullable = true)"," |-- Surface reelle bati: double (nullable = true)"," |-- Nombre pieces principales: double (nullable = true)"," |-- Surface terrain: double (nullable = true)"," |-- Type_voie: string (nullable = true)","","root"," |-- CODGEO_2023: string (nullable = true)"," |-- annee: integer (nullable = true)"," |-- classe: string (nullable = true)"," |-- unité_de_compte: string (nullable = true)"," |-- valeur_publiée: string (nullable = true)"," |-- faits: string (nullable = true)"," |-- tauxpourmille: string (nullable = true)"," |-- complementinfoval: string (nullable = true)"," |-- complementinfotaux: string (nullable = true)"," |-- POP: integer (nullable = true)"," |-- millPOP: integer (nullable = true)"," |-- LOG: string (nullable = true)"," |-- millLOG: integer (nullable = true)","","root"," |-- code_commune_INSEE: string (nullable = true)"," |-- nom_commune_postal: string (nullable = true)"," |-- code_postal: integer (nullable = true)"," |-- libelle_acheminement: string (nullable = true)"," |-- ligne_5: string (nullable = true)"," |-- latitude: double (nullable = true)"," |-- longitude: double (nullable = true)"," |-- code_commune: integer (nullable = true)"," |-- article: string (nullable = true)"," |-- nom_commune: string (nullable = true)"," |-- nom_commune_complet: string (nullable = true)"," |-- code_departement: string (nullable = true)"," |-- nom_departement: string (nullable = true)"," |-- code_region: integer (nullable = true)"," |-- nom_region: string (nullable = true)",""]} 
[2024-07-09 18:25:54] production.ERROR: Error processing data: Failed to execute Python script: root
 |-- CODGEO_2023: string (nullable = true)
 |-- annee: integer (nullable = true)
 |-- classe: string (nullable = true)
 |-- unité.de.compte: string (nullable = true)
 |-- valeur.publiée: string (nullable = true)
 |-- faits: string (nullable = true)
 |-- tauxpourmille: string (nullable = true)
 |-- complementinfoval: string (nullable = true)
 |-- complementinfotaux: string (nullable = true)
 |-- POP: integer (nullable = true)
 |-- millPOP: integer (nullable = true)
 |-- LOG: string (nullable = true)
 |-- millLOG: integer (nullable = true)
root
 |-- code_commune_INSEE: string (nullable = true)
 |-- nom_commune_postal: string (nullable = true)
 |-- code_postal: integer (nullable = true)
 |-- libelle_acheminement: string (nullable = true)
 |-- ligne_5: string (nullable = true)
 |-- latitude: double (nullable = true)
 |-- longitude: double (nullable = true)
 |-- code_commune: integer (nullable = true)
 |-- article: string (nullable = true)
 |-- nom_commune: string (nullable = true)
 |-- nom_commune_complet: string (nullable = true)
 |-- code_departement: string (nullable = true)
 |-- nom_departement: string (nullable = true)
 |-- code_region: integer (nullable = true)
 |-- nom_region: string (nullable = true)
root
 |-- Date mutation: string (nullable = true)
 |-- Nature mutation: string (nullable = true)
 |-- Valeur fonciere: string (nullable = true)
 |-- No voie: double (nullable = true)
 |-- Code voie: string (nullable = true)
 |-- Code postal: double (nullable = true)
 |-- Commune: string (nullable = true)
 |-- Code departement: string (nullable = true)
 |-- Code commune: integer (nullable = true)
 |-- Section: string (nullable = true)
 |-- No plan: integer (nullable = true)
 |-- Nombre de lots: integer (nullable = true)
 |-- Type local: string (nullable = true)
 |-- Surface reelle bati: double (nullable = true)
 |-- Nombre pieces principales: double (nullable = true)
 |-- Surface terrain: double (nullable = true)
 |-- Type_voie: string (nullable = true)
root
 |-- CODGEO_2023: string (nullable = true)
 |-- annee: integer (nullable = true)
 |-- classe: string (nullable = true)
 |-- unité_de_compte: string (nullable = true)
 |-- valeur_publiée: string (nullable = true)
 |-- faits: string (nullable = true)
 |-- tauxpourmille: string (nullable = true)
 |-- complementinfoval: string (nullable = true)
 |-- complementinfotaux: string (nullable = true)
 |-- POP: integer (nullable = true)
 |-- millPOP: integer (nullable = true)
 |-- LOG: string (nullable = true)
 |-- millLOG: integer (nullable = true)
root
 |-- code_commune_INSEE: string (nullable = true)
 |-- nom_commune_postal: string (nullable = true)
 |-- code_postal: integer (nullable = true)
 |-- libelle_acheminement: string (nullable = true)
 |-- ligne_5: string (nullable = true)
 |-- latitude: double (nullable = true)
 |-- longitude: double (nullable = true)
 |-- code_commune: integer (nullable = true)
 |-- article: string (nullable = true)
 |-- nom_commune: string (nullable = true)
 |-- nom_commune_complet: string (nullable = true)
 |-- code_departement: string (nullable = true)
 |-- nom_departement: string (nullable = true)
 |-- code_region: integer (nullable = true)
 |-- nom_region: string (nullable = true)
  
127.0.0.1 -  09/Jul/2024:16:24:08 +0000 "GET /index.php" 500
context canceled
